{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset-prep.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "k-fC5rnxBGrn"
      ],
      "mount_file_id": "1uq2a91KEh6FQ7wsVNsI9AjMr-poUEjVF",
      "authorship_tag": "ABX9TyNRPomHvMhvDPmp4WrwJ46M",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiaweihu08/Chatbot/blob/master/dataset_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-fC5rnxBGrn"
      },
      "source": [
        "# Extracting utterances from datasets\n",
        "\n",
        "These are the dialog dataset from which the training and evaluation utterances are extracted.\n",
        "\n",
        "1. **Daily Dialogues**\n",
        "2. **ConvAI**\n",
        "3. **Empathetic Dialogues**\n",
        "4. **Persona Chat**\n",
        "5. **Cornell Movies Dataset**\n",
        "\n",
        "Conversations are stored in different ways in different datasets, and a function is defined for each to extract the conversations in the form of **[l1, l2, l3, l4, ..., ln]**, where li represents a particular utterance.\n",
        "\n",
        "The obtained conversations are then converted into utterances for training and evaluation. For a given conversation [l1, l2, l3, ..., ln], we extracte utterance pairs of the form **[l1, l2], [l2, l3], ..., [ln-1, ln]**. In each pair, the first line is called **message** and is used as model input, and the second line is called **response** and used as the correct model output.\n",
        "\n",
        "The message and response pairs extracted from each dataset are combined at the end. The **tokenizer** is fit to all the unique utterances obtained (**set(messages + responses)**). **22.0426** utterance pairs are obtained from the above datasets, **10.000** of which are used as evaluation set.\n",
        "\n",
        "Both training and evaluation sets are stored in separate txt files.\n",
        "\n",
        "(CCPE and Holl-E are datasets that mainly focus on movie reviews, thus are excluded.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i81VzBTp-1ak"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "MAX_LEN = 14\n",
        "BUFFER_SIZE = 150000\n",
        "VOCAB_SIZE = 13199 # Eliminating words that appear less than 3 times\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "root_path = '/content/drive/MyDrive/Colab Notebooks/Chatbots/version-2'\n",
        "path_to_datasets = os.path.join(root_path, 'datasets')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wF4_vO7gUJg"
      },
      "source": [
        "file_names = ['training file', 'test file', 'validation file']\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'([.,?!])', r' \\1 ', text)\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z0-9:,.?!]\", ' ', text)\n",
        "\n",
        "    text = re.sub(r\"\\s+\", ' ', text)\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = '<start> ' + text + ' <end>'\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX0eZc3O_WOI"
      },
      "source": [
        "# -------------------- Daily Dialogues --------------------\n",
        "def load_process_and_filter_DD(path_to_file, max_len=MAX_LEN):\n",
        "    with open(path_to_file, 'r') as f:\n",
        "        lines = f.read().strip().split('\\n')\n",
        "    \n",
        "    messages = []\n",
        "    responses = []\n",
        "\n",
        "    for line in lines:\n",
        "        conv = line.strip().split('__eou__')[:-1]\n",
        "        conv = list(map(preprocess_text, conv))\n",
        "        for i in range(len(conv)-1):\n",
        "            m, r = conv[i], conv[i+1]\n",
        "            if m and r and 2 < len(m.split()) <= max_len and 2 < len(r.split()) <= max_len:\n",
        "                messages.append(m)\n",
        "                responses.append(r)\n",
        "\n",
        "    assert len(messages) == len(responses)\n",
        "    print(f'- number of utterances: {len(messages)}\\n')\n",
        "    \n",
        "    return messages, responses\n",
        "\n",
        "\n",
        "# -------------------- ConvAI --------------------\n",
        "def load_process_and_filter_CA(file_path, max_len=MAX_LEN):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    conversations = []\n",
        "\n",
        "    for element in data:\n",
        "        skip = False\n",
        "        for user in element['users']:\n",
        "            if user['userType'] == 'Bot':\n",
        "                skip = True\n",
        "                break\n",
        "        \n",
        "        if skip == False and len(element['thread']) > 1:\n",
        "            conversations.append([preprocess_text(thread['text']) for thread in element['thread']])\n",
        "\n",
        "    print(f'- number of conversations between human users: {len(conversations)}')\n",
        "\n",
        "    messages, responses = [], []\n",
        "\n",
        "    for conv in conversations:\n",
        "        for i in range(len(conv)-1):\n",
        "            m, r = conv[i], conv[i+1]\n",
        "            if m and r and 2 < len(m.split()) <= max_len and 2 < len(r.split()) <= max_len:\n",
        "                messages.append(m)\n",
        "                responses.append(r)\n",
        "\n",
        "    assert len(messages) == len(responses)\n",
        "\n",
        "    print(f'- number of utterances: {len(messages)}\\n')\n",
        "    return messages, responses\n",
        "\n",
        "\n",
        "# -------------------- Empathetic Dialogues --------------------\n",
        "def load_process_and_filter_ED(file_path, max_len=MAX_LEN):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = f.read().strip().split('\\n')\n",
        "\n",
        "    conversations = []\n",
        "    conversation = []\n",
        "    utterance_id = 0\n",
        "    for row in data[1:]:\n",
        "        row = row.split(',')\n",
        "        id, utterance = int(row[1]), preprocess_text(row[5])\n",
        "        if id == utterance_id + 1:\n",
        "            conversation.append(utterance)\n",
        "            utterance_id += 1\n",
        "        else:\n",
        "            conversations.append(conversation)\n",
        "            conversation = []\n",
        "            utterance_id = 1\n",
        "\n",
        "    messages = []\n",
        "    responses = []\n",
        "    for conv in conversations:\n",
        "        for i in range(len(conv) - 1):\n",
        "            m = conv[i]\n",
        "            r = conv[i + 1]\n",
        "            if m and r and 2 < len(m.split()) <= max_len and 2 < len(r.split()) <= max_len:\n",
        "                messages.append(m)\n",
        "                responses.append(r)\n",
        "    \n",
        "    assert len(messages) == len(responses)\n",
        "\n",
        "    print(f'- number of conversations: {len(conversations)}')\n",
        "    print(f'- number of utterances: {len(messages)}\\n')\n",
        "\n",
        "    return messages, responses\n",
        "\n",
        "\n",
        "# -------------------- Persona Chat --------------------\n",
        "def load_process_and_filter_PC(path_to_file, max_len=MAX_LEN):\n",
        "    with open(path_to_file) as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    data['train'].extend(data['valid'])\n",
        "    print(f\"- number of conversations: {len(data['train'])}\")\n",
        "    messages = []\n",
        "    responses = []\n",
        "    for conv in data['train']:\n",
        "        conv = [preprocess_text(utter) for utter in conv['utterances'][-1]['history']]\n",
        "        for i in range(len(conv) - 1):\n",
        "            m, r = conv[i], conv[i+1]\n",
        "            if m and r and 2 < len(m.split()) <= max_len and 2 < len(r.split()) <= max_len:\n",
        "                messages.append(m)\n",
        "                responses.append(r)\n",
        "    \n",
        "    assert len(messages) == len(responses)\n",
        "\n",
        "    print(f'- number of utterances: {len(messages)}\\n')\n",
        "\n",
        "    return messages, responses\n",
        "\n",
        "\n",
        "# -------------------- Cornell Movies Dataset --------------------\n",
        "def load_source_data(path_to_convs, path_to_lines):\n",
        "    with open(path_to_convs, encoding='iso-8859-1') as f:\n",
        "        convs = f.read().strip().split('\\n')\n",
        "\n",
        "    convs = [re.findall(r'L\\d+', conv) for conv in convs]\n",
        "    \n",
        "    with open(path_to_lines, encoding='iso-8859-1') as f:\n",
        "        lines = f.read().strip().split('\\n')\n",
        "    \n",
        "    line_dict = dict()\n",
        "    for line in lines:\n",
        "        line = line.split(' +++$+++ ')\n",
        "        text = preprocess_text(line[-1])\n",
        "\n",
        "        line_dict[line[0]] = text\n",
        "\n",
        "    return convs, line_dict\n",
        "\n",
        "\n",
        "def create_conv_pairs(path_to_convs, path_to_lines, max_len=MAX_LEN):\n",
        "    convs, line_dict = load_source_data(path_to_convs, path_to_lines)\n",
        "    \n",
        "    messages = []\n",
        "    responses = []\n",
        "    for turns in convs:\n",
        "        for i in range(len(turns)-1):\n",
        "            m = line_dict[turns[i]]\n",
        "            r = line_dict[turns[i+1]]\n",
        "            if m and r and 2 < len(m.split()) <= max_len and 2 < len(r.split()) <= max_len:\n",
        "                messages.append(m)\n",
        "                responses.append(r)\n",
        "    \n",
        "    assert len(messages) == len(responses)\n",
        "    print(f'- number of utterances: {len(messages)}')\n",
        "    return messages, responses\n",
        "\n",
        "\n",
        "# # -------------------- Holl-E --------------------\n",
        "# def load_process_and_filter_HE(path_to_file, max_len=MAX_LEN):\n",
        "#     with open(path_to_file) as f:\n",
        "#         data = json.load(f)\n",
        "\n",
        "#     print(f'- number of conversations: {len(data)}')\n",
        "\n",
        "#     messages = []\n",
        "#     responses = []\n",
        "#     for chat in data:\n",
        "#         conv = list(map(preprocess_text, chat['chat']))\n",
        "#         for i in range(len(conv) - 1):\n",
        "#             m, r = conv[i], conv[i+1]\n",
        "#             if m and r and 2 < len(m.split()) <= max_len and 2 < len(r.split()) <= max_len:\n",
        "#                 messages.append(m)\n",
        "#                 responses.append(r)\n",
        "    \n",
        "#     assert len(messages) == len(responses)\n",
        "    \n",
        "#     print(f'- number of utterances: {len(messages)}\\n')\n",
        "    \n",
        "#     return messages, responses\n",
        "\n",
        "\n",
        "# # -------------------- CCPE --------------------\n",
        "# def load_process_and_filter_CCPE(path_to_file, max_len=MAX_LEN):\n",
        "#     with open(path_to_file) as f:\n",
        "#         data = json.load(f)\n",
        "    \n",
        "#     print(f'- number of conversations: {len(data)}')\n",
        "#     messages = []\n",
        "#     responses = []\n",
        "#     for row in data:\n",
        "#         convs = [preprocess_text(utter['text']) for utter in row['utterances']]\n",
        "#         for i in range(len(convs) - 1):\n",
        "#             m , r = convs[i], convs[i + 1]\n",
        "#             if m and r and 2 < len(m.split()) <= max_len and 2 < len(r.split()) <= max_len:\n",
        "#                 messages.append(m)\n",
        "#                 responses.append(r)\n",
        "#     assert len(messages) == len(responses)\n",
        "\n",
        "#     print(f'- number of utterances: {len(messages)}\\n')\n",
        "\n",
        "#     return messages, responses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FntcUnNobMi2"
      },
      "source": [
        "### Extracting utterances and saving to files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-DLpTGajYTm",
        "outputId": "5a9e250e-0ba4-48f9-c7c7-7048879b7bf4"
      },
      "source": [
        "print('-------------------- Daily Dialogues --------------------')\n",
        "train_path_DD = os.path.join(path_to_datasets, 'EMNLP_dataset/train/dialogues_train.txt')\n",
        "test_path_DD = os.path.join(path_to_datasets, 'EMNLP_dataset/test/dialogues_test.txt')\n",
        "valid_path_DD = os.path.join(path_to_datasets, 'EMNLP_dataset/validation/dialogues_validation.txt')\n",
        "\n",
        "\n",
        "file_paths_DD = [train_path_DD, test_path_DD, valid_path_DD]\n",
        "\n",
        "DD_messages, DD_responses = [], []\n",
        "\n",
        "for file_name, file_path in zip(file_names, file_paths_DD):\n",
        "    print(f'Loading from {file_name}...')\n",
        "    messages, responses = load_process_and_filter_DD(file_path)\n",
        "    DD_messages.extend(messages)\n",
        "    DD_responses.extend(responses)\n",
        "\n",
        "assert len(DD_messages) == len(DD_responses)\n",
        "print(f'Total number of utterances from Daily Dialogues: {len(DD_messages)}\\n')\n",
        "\n",
        "for m, r in zip(DD_messages[:5], DD_responses[:5]):\n",
        "    print(m, ' +++ ', r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Daily Dialogues --------------------\n",
            "Loading from training file...\n",
            "- number of utterances: 26731\n",
            "\n",
            "Loading from test file...\n",
            "- number of utterances: 2286\n",
            "\n",
            "Loading from validation file...\n",
            "- number of utterances: 2467\n",
            "\n",
            "Total number of utterances from Daily Dialogues: 31484\n",
            "\n",
            "<start> good . let s go now . <end>  +++  <start> all right . <end>\n",
            "<start> really ? i think that s impossible ! <end>  +++  <start> you mean 30 push ups ? <end>\n",
            "<start> you mean 30 push ups ? <end>  +++  <start> yeah ! <end>\n",
            "<start> can you study with the radio on ? <end>  +++  <start> no , i listen to background music . <end>\n",
            "<start> no , i listen to background music . <end>  +++  <start> what is the difference ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh01qcYqCExA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca892897-660a-4630-8ede-2111d832664a"
      },
      "source": [
        "print('-------------------- ConvAI --------------------')\n",
        "path_to_convai = os.path.join(path_to_datasets, 'ConvAI/train_full.json')\n",
        "\n",
        "CA_messages, CA_responses = load_process_and_filter_CA(path_to_convai)\n",
        "\n",
        "for m, r in zip(CA_messages[:5], CA_responses[:5]):\n",
        "    print(m, ' +++ ', r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- ConvAI --------------------\n",
            "- number of conversations between human users: 404\n",
            "- number of utterances: 3911\n",
            "\n",
            "<start> hi <end>  +++  <start> hi <end>\n",
            "<start> hi <end>  +++  <start> what do you think abouy it ? <end>\n",
            "<start> what do you think abouy it ? <end>  +++  <start> about what ? <end>\n",
            "<start> about what ? <end>  +++  <start> about text <end>\n",
            "<start> what is pipa <end>  +++  <start> ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8T0NpFJPu0G",
        "outputId": "d484594d-b2fa-4a0f-fbe4-8f401d850692"
      },
      "source": [
        "print('-------------------- Empathetic Dialogues --------------------')\n",
        "train_path_ED = os.path.join(path_to_datasets, 'EmpatheticDialogues/train.csv')\n",
        "test_path_ED = os.path.join(path_to_datasets, 'EmpatheticDialogues/test.csv')\n",
        "valid_path_ED = os.path.join(path_to_datasets, 'EmpatheticDialogues/valid.csv')\n",
        "\n",
        "file_paths_ED = [train_path_ED, test_path_ED, valid_path_ED]\n",
        "\n",
        "ED_messages, ED_responses = [], []\n",
        "for file_name, file_path in zip(file_names, file_paths_ED):\n",
        "    print(f'Loading from {file_name}...')\n",
        "    messages, responses = load_process_and_filter_ED(file_path)\n",
        "    ED_messages.extend(messages)\n",
        "    ED_responses.extend(responses)\n",
        "\n",
        "assert len(ED_messages) == len(ED_responses)\n",
        "\n",
        "print(f'Total number of utterances from Empathetic Dialogues: {len(ED_messages)}\\n')\n",
        "\n",
        "for m, r in zip(ED_messages[:5], ED_responses[:5]):\n",
        "    print(m, ' +++ ', r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Empathetic Dialogues --------------------\n",
            "Loading from training file...\n",
            "- number of conversations: 19532\n",
            "- number of utterances: 10163\n",
            "\n",
            "Loading from test file...\n",
            "- number of conversations: 2546\n",
            "- number of utterances: 658\n",
            "\n",
            "Loading from validation file...\n",
            "- number of conversations: 2769\n",
            "- number of utterances: 1012\n",
            "\n",
            "Total number of utterances from Empathetic Dialogues: 11833\n",
            "\n",
            "<start> this was a best friend . i miss her . <end>  +++  <start> where has she gone ? <end>\n",
            "<start> where has she gone ? <end>  +++  <start> we no longer talk . <end>\n",
            "<start> we no longer talk . <end>  +++  <start> oh was this something that happened because of an argument ? <end>\n",
            "<start> oh ya ? i don t really see how <end>  +++  <start> dont you feel so . . its a wonder <end>\n",
            "<start> i virtually thought so . . and i used to get sweatings <end>  +++  <start> wait what are sweatings <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dvAyLenSqzw",
        "outputId": "423511a3-5ecf-4317-f42f-07e30056c6fa"
      },
      "source": [
        "print('-------------------- Persona Chat --------------------')\n",
        "path_to_PC = os.path.join(path_to_datasets, 'Persona_Chat/personachat_original.json')\n",
        "\n",
        "print('Loading Persona Chat data...')\n",
        "PC_messages, PC_responses = load_process_and_filter_PC(path_to_PC)\n",
        "\n",
        "for m, r in zip(PC_messages[:5], PC_responses[:5]):\n",
        "    print(m, ' +++ ', r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Persona Chat --------------------\n",
            "Loading Persona Chat data...\n",
            "- number of conversations: 18878\n",
            "- number of utterances: 83908\n",
            "\n",
            "<start> wow , four sisters . just watching game of thrones . <end>  +++  <start> that is a good show i watch that while drinking iced tea <end>\n",
            "<start> that is a good show i watch that while drinking iced tea <end>  +++  <start> i agree . what do you do for a living ? <end>\n",
            "<start> i enjoy hanging with my mother she s my best friend <end>  +++  <start> that s nice . moms are pretty cool too . <end>\n",
            "<start> hi ! i work as a gourmet cook . <end>  +++  <start> i don t like carrots . i throw them away . <end>\n",
            "<start> i don t like carrots . i throw them away . <end>  +++  <start> really . but , i can sing pitch perfect . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSxP7WcgSqcr"
      },
      "source": [
        "# print('-------------------- Holl-E --------------------')\n",
        "# train_path_HE = os.path.join(path_to_datasets, 'Holl-E/train_data.json')\n",
        "# test_path_HE = os.path.join(path_to_datasets, 'Holl-E/test_data.json')\n",
        "# valid_path_HE = os.path.join(path_to_datasets, 'Holl-E/dev_data.json')\n",
        "\n",
        "# file_paths_HE = [train_path_HE, test_path_HE, valid_path_HE]\n",
        "\n",
        "# HE_messages, HE_responses = [], []\n",
        "# for file_name, file_path in zip(file_names, file_paths_HE):\n",
        "#     print(f'Loading from {file_name}...')\n",
        "#     messages, responses = load_process_and_filter_HE(file_path)\n",
        "#     HE_messages.extend(messages)\n",
        "#     HE_responses.extend(responses)\n",
        "\n",
        "# assert len(HE_messages) == len(HE_responses)\n",
        "\n",
        "# print(f'Total number of utterances from Holl-E: {len(HE_messages)}\\n')\n",
        "\n",
        "# for m, r in zip(HE_messages[:5], HE_responses[:5]):\n",
        "#     print(m, ' +++ ', r)\n",
        "\n",
        "\n",
        "# print('-------------------- CCPE --------------------')\n",
        "# path_to_ccpe = os.path.join(path_to_datasets, 'Coached Conversational Preference Elicitation (CCPE)/data.json')\n",
        "\n",
        "# print('Loading CCPE data...')\n",
        "# CCPE_messages, CCPE_responses = load_process_and_filter_CCPE(path_to_ccpe)\n",
        "\n",
        "# for m, r in zip(CCPE_messages[:5], CCPE_responses[:5]):\n",
        "#     print(m, ' +++ ', r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0WfSdHnLMxT",
        "outputId": "17ecf137-1928-4a52-b1f5-bcb542c661c9"
      },
      "source": [
        "print('-------------------- Cornell Movies Dataset --------------------')\n",
        "cornell_folder = '/content/drive/My Drive/Colab Notebooks/Chatbots/version-1/cornell movie-dialogs corpus'\n",
        "\n",
        "path_to_convs = os.path.join(cornell_folder, 'movie_conversations.txt')\n",
        "path_to_lines = os.path.join(cornell_folder, 'movie_lines.txt')\n",
        "\n",
        "cornell_messages, cornell_responses = create_conv_pairs(path_to_convs, path_to_lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Cornell Movies Dataset --------------------\n",
            "- number of utterances: 89290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFY5vGqK35GE",
        "outputId": "0a782692-af6b-4b18-adfd-5697f5773019"
      },
      "source": [
        "all_messages = DD_messages + CA_messages + ED_messages + PC_messages + cornell_messages\n",
        "all_responses = DD_responses + CA_responses + ED_responses + PC_responses + cornell_responses\n",
        "\n",
        "assert len(all_messages) == len(all_responses)\n",
        "\n",
        "print(f'Total number of utterances: {len(all_messages)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of utterances: 220426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdOo8uDgMGMz"
      },
      "source": [
        "def save_tokenizer(tokenizer):\n",
        "    file_name = os.path.join(root_path, 'tokenizer.json')\n",
        "    tokenizer_json = tokenizer.to_json()\n",
        "    with open(file_name, 'w', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "\n",
        "def save_utterances(messages, responses, vocab_size=VOCAB_SIZE):\n",
        "    train_utters_path = os.path.join(path_to_datasets, 'train_utters.txt')\n",
        "    eval_utters_path = os.path.join(path_to_datasets, 'test_utters.txt')\n",
        "    \n",
        "    all_utterances = set(messages + responses)\n",
        "    tokenizer = tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, filters='')\n",
        "    tokenizer.fit_on_texts(all_utterances)\n",
        "    save_tokenizer(tokenizer)\n",
        "\n",
        "    train_m, eval_m, train_r, eval_r = train_test_split(messages, responses,\n",
        "                                                        test_size=10000, random_state=42)\n",
        "    \n",
        "    breaker = ' _+++_ '\n",
        "    with open(train_utters_path, 'w') as f:\n",
        "        for i in range(len(train_m)):\n",
        "            f.write(train_m[i] + breaker + train_r[i] + '\\n')\n",
        "\n",
        "    with open(eval_utters_path, 'w') as f:\n",
        "        for i in range(len(eval_m)):\n",
        "            f.write(eval_m[i] + breaker + eval_r[i] + '\\n')\n",
        "\n",
        "    print(f'- number of training instances: {len(train_m)}')\n",
        "    print(f'- number of evaluation instances: {len(eval_m)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB5SKTJFF_m2",
        "outputId": "bc4090ce-7f2c-4fdf-ed10-6c83af69b708"
      },
      "source": [
        "save_utterances(all_messages, all_responses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- number of training instances: 210426\n",
            "- number of evaluation instances: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}