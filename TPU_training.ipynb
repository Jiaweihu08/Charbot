{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU training",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1w04G_8cBydZjQetZcpp5fz7oKpf3i6k0",
      "authorship_tag": "ABX9TyM1aVdHGOK9BxMl4KRSC9rn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiaweihu08/Chatbot/blob/master/TPU_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P67y8N3HxAzv"
      },
      "source": [
        "# Building a chitchating Chatbot using a seq2seq model with attention\n",
        "\n",
        "##### The Chatbot to be built in this notebook is an **encoder-decoder based seq2seq model**. Both encoder and decoder are **RNN** models with two layers of **GRU** cells, the RNN layers in the encoder are  **bidirectional** and the decoder uses **Loung's attention** to improve performance. Both encoder and decoder share the same **embedding** layer for their inputs.\n",
        "\n",
        " The dataset that the model is trained on is a combination of the following conversational datasets:\n",
        "- **DailyDialogues**\n",
        "- **ConvAI**\n",
        "- **EmpatheticDialogues**\n",
        "- **Persona Chat**\n",
        "- **Cornell Movie's Dataset**\n",
        "\n",
        "The steps of combining these dataset are conducted in a different notebook. The combined dataset has **220426** utterances in total, and it's split into training and evaluation sets, **10.000** instances are used for evaluation.\n",
        "\n",
        "The model is trained with **TPUs** available on Google Colab. **Beam-Search** is used for inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmbto8hck_kO"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from random import choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2npDM5C1ULY"
      },
      "source": [
        "Set up to use TPU for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ3OZmK6lqE_",
        "outputId": "6f0940f3-d33a-478f-df3e-ce6f5b3ce1e1"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.67.39.154:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.67.39.154:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1r4xhdnlzie",
        "outputId": "019bb14b-6058-4d9f-9a01-85f28be7abb9"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82mhu58W1aEK"
      },
      "source": [
        "Defining parameter:\n",
        "- The max length of the sentences is 14, the vocab size is set to eliminate words that appeared less than 3 times in the entire dataset. These parameters are set during the creation of the dataset.\n",
        "\n",
        "- The global batch size is 64, and there are 8 TPU's used for distributed training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3gbVBC8_Tyq"
      },
      "source": [
        "### Loading the dataset and the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57QIki6Vvers"
      },
      "source": [
        "MAX_LEN = 14\n",
        "BUFFER_SIZE = 150000\n",
        "VOCAB_SIZE = 13199 # Eliminating words that appear less than 3 times\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 8\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "# strategy.num_replicas_in_sync ==> 8\n",
        "\n",
        "root_path = '/content/drive/MyDrive/Colab Notebooks/Chatbots/version-2'\n",
        "path_to_datasets = os.path.join(root_path, 'datasets')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6TDumFS2Euk"
      },
      "source": [
        "The following block of code defines the function for dataset and tokenizer loading. Both dataset and tokenizer are extracted and defined in a separate notebook.\n",
        "\n",
        "The loaded dataset is defined as tensorflow's Dataset object, which is a prerequisite for TPU usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JofgpVvPvTKw"
      },
      "source": [
        "def load_dataset(path_to_dataset):\n",
        "    with open(path_to_dataset) as f:\n",
        "        lines = f.read().strip().split('\\n')\n",
        "\n",
        "    messages = []\n",
        "    responses = []\n",
        "    breaker = ' _+++_ '\n",
        "    for line in lines:\n",
        "        m, r = line.split(breaker)\n",
        "        messages.append(m)\n",
        "        responses.append(r)\n",
        "    \n",
        "    print(f'- number of instances: {len(messages)}')\n",
        "\n",
        "    return messages, responses\n",
        "\n",
        "\n",
        "def load_tokenizer():\n",
        "    tokenizer_dir = os.path.join(root_path, 'tokenizer.json')\n",
        "    with open(tokenizer_dir) as f:\n",
        "        data = json.load(f)\n",
        "        tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def get_tensors(message, response, tokenizer, max_len):\n",
        "    m_tensor = tokenizer.texts_to_sequences(message)\n",
        "    m_tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        m_tensor,\n",
        "        maxlen = max_len,\n",
        "        padding='post',\n",
        "        truncating='post')\n",
        "    \n",
        "    r_tensor = tokenizer.texts_to_sequences(response)\n",
        "    r_tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        r_tensor,\n",
        "        maxlen=max_len,\n",
        "        padding='post',\n",
        "        truncating='post')\n",
        "\n",
        "    return m_tensor, r_tensor\n",
        "\n",
        "\n",
        "def get_dataset(path_to_dataset, tokenizer, max_len=MAX_LEN,\n",
        "                buffer_size=BUFFER_SIZE, batch_size=GLOBAL_BATCH_SIZE):\n",
        "    \n",
        "    messages, responses = load_dataset(path_to_dataset)\n",
        "    steps_per_epoch = len(messages) // batch_size\n",
        "\n",
        "    message_tensor, response_tensor = get_tensors(messages, responses, tokenizer, max_len)\n",
        "    \n",
        "    print(f'- tensor shape: {message_tensor.shape}')\n",
        "    print(f'- steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((message_tensor, response_tensor))\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset, steps_per_epoch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaafgYwAvXsz",
        "outputId": "17971df2-9c4b-48de-901f-0f8543afc959"
      },
      "source": [
        "train_utters_path = os.path.join(path_to_datasets, 'train_utters.txt')\n",
        "eval_utters_path = os.path.join(path_to_datasets, 'eval_utters.txt')\n",
        "\n",
        "tokenizer = load_tokenizer()\n",
        "\n",
        "print(\"Loading training data...\")\n",
        "train_set, steps_per_epoch = get_dataset(train_utters_path, tokenizer)\n",
        "\n",
        "print(\"\\nLoading evaluation data...\")\n",
        "eval_set, steps_per_epoch_eval = get_dataset(eval_utters_path, tokenizer, batch_size=64)\n",
        "\n",
        "\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_set)\n",
        "eval_dist_dataset = strategy.experimental_distribute_dataset(eval_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "- number of instances: 210426\n",
            "- tensor shape: (210426, 14)\n",
            "- steps per epoch: 3287\n",
            "\n",
            "Loading evaluation data...\n",
            "- number of instances: 10000\n",
            "- tensor shape: (10000, 14)\n",
            "- steps per epoch: 156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6okpHZdg_Xf1"
      },
      "source": [
        "### Model training\n",
        "\n",
        "Brief description of the model:\n",
        "- The model is a **encoder-decoder** based RNN model.\n",
        "- The encoder is in charge of **encoding** the input sequence and passes this sequence together with its **last hidden states** to the decoder.\n",
        "- The encoder has a **embedding** layer to map the input token ids to their distributed representations(i.e. vectors), this embedding layer is **shared** with the decoder.\n",
        "- The encoder has two **bidirectional** GRU layers so it can read the inputs **from both direction** to produce better encodings.\n",
        "- The encoded sequences from both direction are **concatenated**, so are the forward and backward hidden states.\n",
        "- The concatenated hidden states are used as hidden state initializers for the decoder.\n",
        "- Since the encoder is concatenating outputs from both directions, its number of **units** in its GRU layers are the **half** of that of the decoder.\n",
        "- The encoded sequence from the encoder is passed to the decode at each time step and an **attention mechanism** is used for information retrieval. The output of the attention is known at the **context vector**. In the case of **Loung's attention**, this context and the encoder's last RNN output are **concatenated** and passed to a dense layer to produce predictions.\n",
        "- Aside from the encoder outputs, the decoder also takes an input that is its prediction from the last time step(during inference). During training, the correct output from the last step is used instead. This is know as **teacher forcing**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCGFhgxb0Dw4"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, GRU, Bidirectional, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, enc_units, embedding_dim, vocab_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.gru_1 = Bidirectional(\n",
        "            GRU(enc_units,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                dropout=0.1,\n",
        "                recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        self.gru_2 = Bidirectional(\n",
        "            GRU(enc_units,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "    def call(self, enc_input):\n",
        "        # enc_input shape: (batch_size, max_len)\n",
        "        # x_emb shape: (batch_size, max_len, embedding_dim)\n",
        "        x_emb = self.embedding(enc_input)\n",
        "\n",
        "        # sequence shape: (batch_size, max_len, 2 * units)\n",
        "        # hiddens shape: (batch_size, 2 * units)\n",
        "        sequence_1, hidden_forward_1, hidden_backward_1 = self.gru_1(x_emb)\n",
        "        hiddens_1 = tf.concat([hidden_forward_1, hidden_backward_1], axis=-1)\n",
        "\n",
        "        output_sequence, hidden_forward_2, hidden_backward_2 = self.gru_2(sequence_1)\n",
        "        hiddens_2 = tf.concat([hidden_forward_2, hidden_backward_2], axis=-1)\n",
        "\n",
        "        hiddens = [hiddens_1, hiddens_2]\n",
        "\n",
        "        return output_sequence, hiddens\n",
        "\n",
        "\n",
        "class LuongAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.W = Dense(units)\n",
        "\n",
        "    def call(self, attention_inputs):\n",
        "        # query shape: (batch_size, 1, units)\n",
        "        # values shape: (batch_size, max_len, units)\n",
        "        query, values = attention_inputs\n",
        "\n",
        "        # scores shape: (batch_size, 1, max_len)\n",
        "        scores = tf.matmul(query, self.W(values), transpose_b=True)\n",
        "\n",
        "        # attention_weights shape: (batch_size, 1, max_len)\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # context_vector shape: (batch_size, 1, units)\n",
        "        context_vector = tf.matmul(attention_weights, values)\n",
        "        \n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, dec_units, embedding_layer, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.embedding = embedding_layer\n",
        "        \n",
        "        self.gru_1 = GRU(dec_units,\n",
        "                         return_sequences=True,\n",
        "                         return_state=True,\n",
        "                         dropout=0.1,\n",
        "                         recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        self.gru_2 = GRU(dec_units,\n",
        "                         return_sequences=True,\n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        self.attention = LuongAttention(dec_units)\n",
        "\n",
        "        self.fc = Dense(dec_units, activation='tanh')\n",
        "        self.out = Dense(vocab_size)\n",
        "        \n",
        "        \n",
        "    def call(self, dec_inputs):\n",
        "        # x shape: (batch_size, 1)\n",
        "        # hiddens shape: (batch_size, units)\n",
        "        # enc_outputs shape: (batch_size, max_len, units)\n",
        "        x, hiddens, enc_output = dec_inputs\n",
        "        input_hiddens_1, input_hiddens_2 = hiddens\n",
        "\n",
        "        # x_emb shape: (batch_size, 1, embedding_dim)\n",
        "        x_emb = self.embedding(x)\n",
        "\n",
        "        # output sequence shape: (batch_size, 1, units)\n",
        "        # hiddens shape: (batch_size, units)\n",
        "        sequence_1, hiddens_1 = self.gru_1(x_emb, initial_state=input_hiddens_1)\n",
        "        output_sequence, hiddens_2 = self.gru_2(sequence_1, initial_state=input_hiddens_2)\n",
        "\n",
        "        hiddens = [hiddens_1, hiddens_2]\n",
        "\n",
        "        # context shape: (batch_size, 1, units)\n",
        "        # attention_weights shape: (batch_size, 1, max_len)\n",
        "        attention_inputs = (output_sequence, enc_output)\n",
        "        context, attention_weights = self.attention(attention_inputs)\n",
        "        \n",
        "        # output_sequence shape: (batch_size, 2 * units)\n",
        "        output_sequence = tf.concat([context, output_sequence], -1)\n",
        "        output_sequence = tf.squeeze(output_sequence, axis=1)\n",
        "\n",
        "        # output_sequence shape: (batch_size, units)\n",
        "        output_sequence = self.fc(output_sequence)\n",
        "\n",
        "        # logits shape: (batch_size, VOCAB_SIZE)\n",
        "        logits = self.out(output_sequence)\n",
        "        \n",
        "        return logits, hiddens, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNZeB3N18fzO"
      },
      "source": [
        "Defining model, optimizer, loss function, and checkpoint for saving.\n",
        "\n",
        "This has to be done with the context manager **strategy.scope()** for TPU usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsqJbI060_YN"
      },
      "source": [
        "units = 1024\n",
        "embedding_dim = 512\n",
        "\n",
        "checkpoint_dir = os.path.join(root_path, 'training_checkpoints_TPU')\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "\n",
        "with strategy.scope():\n",
        "    encoder = Encoder(units//2, embedding_dim, VOCAB_SIZE)\n",
        "    decoder = Decoder(units, encoder.embedding, VOCAB_SIZE)\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                    encoder=encoder,\n",
        "                                    decoder=decoder)\n",
        "\n",
        "    loss_object = SparseCategoricalCrossentropy(\n",
        "        from_logits=True,\n",
        "        reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "    def loss_function(real, pred):\n",
        "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "        loss_ = loss_object(real, pred)\n",
        "\n",
        "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "        loss_ *= mask\n",
        "\n",
        "        return tf.nn.compute_average_loss(loss_, global_batch_size=GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zELealf85HQ"
      },
      "source": [
        "Defining training and evalution steps:\n",
        "\n",
        "- As mentioned before, the inputs are first encoded by the encoder and it's used at each time step by the decoder. The encoder's last hidden state from each of it GRU layers are used as hidden state initializer for the first time step of the decoder.\n",
        "\n",
        "- The loss is computed at each time step depending on how much the predictions and the 'labels' differ.\n",
        "\n",
        "- For gradient calculation, the context manager **tf.GradientTape()** is used to track the operations done in the forward pass(from input to output).\n",
        "\n",
        "- **model.fit()** cannot be used in this case since the model is not **auto-regressive**, and the training loop must be defined according to needs.\n",
        "\n",
        "- The decorator @tf.function is used to speed up the operations of the function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvRZDfM80c8o"
      },
      "source": [
        "def train_step(m, r):\n",
        "    loss = 0.0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        enc_out, hiddens = encoder(m)\n",
        "\n",
        "        dec_in = (tf.expand_dims([tokenizer.word_index['<start>']] * r.shape[0], 1),\n",
        "                  hiddens, enc_out)\n",
        "        \n",
        "        for t in range(1, r.shape[1]):\n",
        "\n",
        "            predictions, hiddens, _ = decoder(dec_in)\n",
        "\n",
        "            loss += loss_function(r[:, t], predictions)\n",
        "\n",
        "            dec_in = (tf.expand_dims(r[:, t], 1), hiddens, enc_out)\n",
        "    \n",
        "    batch_loss = (loss / int(r.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    \n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "\n",
        "def eval_step(m, r):\n",
        "    loss = 0.0\n",
        "\n",
        "    enc_out, hiddens = encoder(m)\n",
        "\n",
        "    dec_in = (tf.expand_dims([tokenizer.word_index['<start>']] * r.shape[0], 1),\n",
        "              hiddens, enc_out)\n",
        "    \n",
        "    for t in range(1, r.shape[1]):\n",
        "        predictions, hiddens, _ = decoder(dec_in)\n",
        "\n",
        "        loss += loss_function(r[:, t], predictions)\n",
        "\n",
        "        dec_in = (tf.expand_dims(r[:, t], 1), hiddens, enc_out)\n",
        "\n",
        "    batch_loss = (loss / int(r.shape[1]))\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def distributed_train_step(m, r):\n",
        "    per_replica_losses = strategy.run(train_step, args=(m, r))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                            axis=None)\n",
        "    \n",
        "\n",
        "@tf.function\n",
        "def distributed_eval_step(m, r):\n",
        "    per_replica_losses = strategy.run(eval_step, args=(m, r))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                           axis=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ld3Gc4X0EAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d5fce9-42b9-42a1-e8e5-5b731872e8d0"
      },
      "source": [
        "init_epoch = 0\n",
        "EPOCHS = 20\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "print('Starting training at {}'.format(time.strftime(\"%Y_%m_%d-%H_%M_%S\")))\n",
        "print('Entering training loop...')\n",
        "print('Total Epochs {}'.format(EPOCHS))\n",
        "\n",
        "for epoch in range(init_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # Training Iterations\n",
        "    total_train_loss = 0.0\n",
        "    num_train_batches = 0\n",
        "    for (m, r) in train_dist_dataset:\n",
        "        total_train_loss += distributed_train_step(m, r)\n",
        "        num_train_batches += 1\n",
        "\n",
        "        if num_train_batches % 500 == 0:\n",
        "            print(\">>> Iteration: {}/{}, Train Loss: {:.4f}\".format(\n",
        "                num_train_batches,\n",
        "                steps_per_epoch,\n",
        "                total_train_loss.numpy() / num_train_batches))\n",
        "\n",
        "    train_loss = total_train_loss.numpy() / num_train_batches\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluation Iterations\n",
        "    total_eval_loss= 0.0\n",
        "    num_eval_batches = 0\n",
        "    for (m, r) in eval_dist_dataset:\n",
        "        total_eval_loss += distributed_eval_step(m, r)\n",
        "        num_eval_batches += 1\n",
        "\n",
        "    eval_loss = total_eval_loss.numpy() / num_eval_batches\n",
        "    eval_losses.append(eval_loss)\n",
        "\n",
        "    if epoch + 1 % 2 == 0:\n",
        "        checkpoint.write(checkpoint_prefix, options=local_device_option)\n",
        "\n",
        "\n",
        "    template = \"Epoch {}/{}, Train Loss: {:.4f}, Eval Loss: {:.4f}\"\n",
        "    print(template.format(epoch, EPOCHS, train_loss, eval_loss))\n",
        "    print(\"Time taken for 1 epoch: {}s\".format(int(time.time() - start)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training at 2020_11_28-19_12_52\n",
            "Entering training loop...\n",
            "Total Epochs 20\n",
            ">>> Iteration: 500/3287, Train Loss: 3.2070\n",
            ">>> Iteration: 1000/3287, Train Loss: 3.0026\n",
            ">>> Iteration: 1500/3287, Train Loss: 2.8647\n",
            ">>> Iteration: 2000/3287, Train Loss: 2.7646\n",
            ">>> Iteration: 2500/3287, Train Loss: 2.6920\n",
            ">>> Iteration: 3000/3287, Train Loss: 2.6332\n",
            "Epoch 0/20, Train Loss: 2.6056, Eval Loss: 2.2780\n",
            "Time taken for 1 epoch: 269s\n",
            ">>> Iteration: 500/3287, Train Loss: 2.2795\n",
            ">>> Iteration: 1000/3287, Train Loss: 2.2559\n",
            ">>> Iteration: 1500/3287, Train Loss: 2.2439\n",
            ">>> Iteration: 2000/3287, Train Loss: 2.2287\n",
            ">>> Iteration: 2500/3287, Train Loss: 2.2178\n",
            ">>> Iteration: 3000/3287, Train Loss: 2.2092\n",
            "Epoch 1/20, Train Loss: 2.2050, Eval Loss: 2.1331\n",
            "Time taken for 1 epoch: 229s\n",
            ">>> Iteration: 500/3287, Train Loss: 2.1139\n",
            ">>> Iteration: 1000/3287, Train Loss: 2.1071\n",
            ">>> Iteration: 1500/3287, Train Loss: 2.1057\n",
            ">>> Iteration: 2000/3287, Train Loss: 2.0996\n",
            ">>> Iteration: 2500/3287, Train Loss: 2.0960\n",
            ">>> Iteration: 3000/3287, Train Loss: 2.0924\n",
            "Epoch 2/20, Train Loss: 2.0889, Eval Loss: 2.0687\n",
            "Time taken for 1 epoch: 228s\n",
            ">>> Iteration: 500/3287, Train Loss: 2.0303\n",
            ">>> Iteration: 1000/3287, Train Loss: 2.0240\n",
            ">>> Iteration: 1500/3287, Train Loss: 2.0222\n",
            ">>> Iteration: 2000/3287, Train Loss: 2.0194\n",
            ">>> Iteration: 2500/3287, Train Loss: 2.0162\n",
            ">>> Iteration: 3000/3287, Train Loss: 2.0130\n",
            "Epoch 3/20, Train Loss: 2.0110, Eval Loss: 2.0264\n",
            "Time taken for 1 epoch: 226s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.9582\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.9579\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.9557\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.9540\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.9516\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.9490\n",
            "Epoch 4/20, Train Loss: 1.9470, Eval Loss: 1.9869\n",
            "Time taken for 1 epoch: 227s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.8836\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.8867\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.8880\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.8904\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.8906\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.8909\n",
            "Epoch 5/20, Train Loss: 1.8900, Eval Loss: 1.9553\n",
            "Time taken for 1 epoch: 228s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.8353\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.8381\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.8405\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.8395\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.8378\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.8386\n",
            "Epoch 6/20, Train Loss: 1.8364, Eval Loss: 1.9337\n",
            "Time taken for 1 epoch: 231s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.7825\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.7831\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.7860\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.7858\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.7858\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.7849\n",
            "Epoch 7/20, Train Loss: 1.7846, Eval Loss: 1.9148\n",
            "Time taken for 1 epoch: 230s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.7331\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.7335\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.7320\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.7325\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.7326\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.7340\n",
            "Epoch 8/20, Train Loss: 1.7340, Eval Loss: 1.8937\n",
            "Time taken for 1 epoch: 227s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.6753\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.6766\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.6787\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.6788\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.6794\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.6819\n",
            "Epoch 9/20, Train Loss: 1.6827, Eval Loss: 1.8801\n",
            "Time taken for 1 epoch: 227s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.6162\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.6236\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.6262\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.6277\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.6277\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.6290\n",
            "Epoch 10/20, Train Loss: 1.6305, Eval Loss: 1.8672\n",
            "Time taken for 1 epoch: 227s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.5663\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.5719\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.5715\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.5734\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.5761\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.5766\n",
            "Epoch 11/20, Train Loss: 1.5772, Eval Loss: 1.8560\n",
            "Time taken for 1 epoch: 227s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.5066\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.5112\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.5145\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.5178\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.5213\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.5226\n",
            "Epoch 12/20, Train Loss: 1.5229, Eval Loss: 1.8410\n",
            "Time taken for 1 epoch: 227s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.4528\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.4594\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.4603\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.4612\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.4652\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.4662\n",
            "Epoch 13/20, Train Loss: 1.4675, Eval Loss: 1.8318\n",
            "Time taken for 1 epoch: 227s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.4023\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.4047\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.4063\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.4084\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.4099\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.4112\n",
            "Epoch 14/20, Train Loss: 1.4111, Eval Loss: 1.8229\n",
            "Time taken for 1 epoch: 229s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.3302\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.3400\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.3444\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.3479\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.3498\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.3523\n",
            "Epoch 15/20, Train Loss: 1.3537, Eval Loss: 1.8127\n",
            "Time taken for 1 epoch: 229s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.2796\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.2860\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.2884\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.2909\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.2942\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.2954\n",
            "Epoch 16/20, Train Loss: 1.2960, Eval Loss: 1.8095\n",
            "Time taken for 1 epoch: 228s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.2204\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.2237\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.2284\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.2324\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.2342\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.2365\n",
            "Epoch 17/20, Train Loss: 1.2380, Eval Loss: 1.7991\n",
            "Time taken for 1 epoch: 229s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.1633\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.1666\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.1715\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.1742\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.1766\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.1786\n",
            "Epoch 18/20, Train Loss: 1.1795, Eval Loss: 1.7974\n",
            "Time taken for 1 epoch: 229s\n",
            ">>> Iteration: 500/3287, Train Loss: 1.0962\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.1035\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.1089\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.1146\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.1179\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.1204\n",
            "Epoch 19/20, Train Loss: 1.1215, Eval Loss: 1.7972\n",
            "Time taken for 1 epoch: 229s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Sv6r6OWoPK6",
        "outputId": "1378bda6-a9cc-4291-d60d-02a33fd6d0bd"
      },
      "source": [
        "init_epoch = 20\n",
        "EPOCHS_1 = 31\n",
        "\n",
        "train_losses_1 = []\n",
        "eval_losses_1 = []\n",
        "\n",
        "print('Continue training at {}'.format(time.strftime(\"%Y_%m_%d-%H_%M_%S\")))\n",
        "print('Total Epochs {}'.format(EPOCHS_1 - init_epoch))\n",
        "\n",
        "for epoch in range(init_epoch, EPOCHS_1):\n",
        "    start = time.time()\n",
        "    \n",
        "    # Training Iterations\n",
        "    total_train_loss = 0.0\n",
        "    num_train_batches = 0\n",
        "    for (m, r) in train_dist_dataset:\n",
        "        total_train_loss += distributed_train_step(m, r)\n",
        "        num_train_batches += 1\n",
        "\n",
        "        if num_train_batches % 500 == 0:\n",
        "            print(\">>> Iteration: {}/{}, Train Loss: {:.4f}\".format(\n",
        "                num_train_batches,\n",
        "                steps_per_epoch,\n",
        "                total_train_loss.numpy() / num_train_batches))\n",
        "\n",
        "    train_loss = total_train_loss.numpy() / num_train_batches\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluation Iterations\n",
        "    total_eval_loss= 0.0\n",
        "    num_eval_batches = 0\n",
        "    for (m, r) in eval_dist_dataset:\n",
        "        total_eval_loss += distributed_eval_step(m, r)\n",
        "        num_eval_batches += 1\n",
        "\n",
        "    eval_loss = total_eval_loss.numpy() / num_eval_batches\n",
        "    eval_losses.append(eval_loss)\n",
        "\n",
        "    if epoch + 1 % 2 == 0:\n",
        "        checkpoint.write(checkpoint_prefix, options=local_device_option)\n",
        "\n",
        "\n",
        "    template = \"Epoch {}/{}, Train Loss: {:.4f}, Eval Loss: {:.4f}\"\n",
        "    print(template.format(epoch, EPOCHS, train_loss, eval_loss))\n",
        "    print(\"Time taken for 1 epoch: {}s\".format(int(time.time() - start)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Continue training at 2020_11_28-20_57_08\n",
            "Total Epochs 11\n",
            ">>> Iteration: 500/3287, Train Loss: 1.0423\n",
            ">>> Iteration: 1000/3287, Train Loss: 1.0474\n",
            ">>> Iteration: 1500/3287, Train Loss: 1.0507\n",
            ">>> Iteration: 2000/3287, Train Loss: 1.0544\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.0596\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.0623\n",
            "Epoch 20/20, Train Loss: 1.0641, Eval Loss: 1.7884\n",
            "Time taken for 1 epoch: 227s\n",
            ">>> Iteration: 500/3287, Train Loss: 0.9809\n",
            ">>> Iteration: 1000/3287, Train Loss: 0.9911\n",
            ">>> Iteration: 1500/3287, Train Loss: 0.9956\n",
            ">>> Iteration: 2000/3287, Train Loss: 0.9999\n",
            ">>> Iteration: 2500/3287, Train Loss: 1.0034\n",
            ">>> Iteration: 3000/3287, Train Loss: 1.0068\n",
            "Epoch 21/20, Train Loss: 1.0076, Eval Loss: 1.7929\n",
            "Time taken for 1 epoch: 227s\n",
            ">>> Iteration: 500/3287, Train Loss: 0.9266\n",
            ">>> Iteration: 1000/3287, Train Loss: 0.9332\n",
            ">>> Iteration: 1500/3287, Train Loss: 0.9391\n",
            ">>> Iteration: 2000/3287, Train Loss: 0.9441\n",
            ">>> Iteration: 2500/3287, Train Loss: 0.9474\n",
            ">>> Iteration: 3000/3287, Train Loss: 0.9499\n",
            "Epoch 22/20, Train Loss: 0.9517, Eval Loss: 1.7922\n",
            "Time taken for 1 epoch: 228s\n",
            ">>> Iteration: 500/3287, Train Loss: 0.8723\n",
            ">>> Iteration: 1000/3287, Train Loss: 0.8775\n",
            ">>> Iteration: 1500/3287, Train Loss: 0.8814\n",
            ">>> Iteration: 2000/3287, Train Loss: 0.8864\n",
            ">>> Iteration: 2500/3287, Train Loss: 0.8907\n",
            ">>> Iteration: 3000/3287, Train Loss: 0.8948\n",
            "Epoch 23/20, Train Loss: 0.8972, Eval Loss: 1.7972\n",
            "Time taken for 1 epoch: 228s\n",
            ">>> Iteration: 500/3287, Train Loss: 0.8228\n",
            ">>> Iteration: 1000/3287, Train Loss: 0.8283\n",
            ">>> Iteration: 1500/3287, Train Loss: 0.8317\n",
            ">>> Iteration: 2000/3287, Train Loss: 0.8351\n",
            ">>> Iteration: 2500/3287, Train Loss: 0.8395\n",
            ">>> Iteration: 3000/3287, Train Loss: 0.8442\n",
            "Epoch 24/20, Train Loss: 0.8458, Eval Loss: 1.8004\n",
            "Time taken for 1 epoch: 228s\n",
            ">>> Iteration: 500/3287, Train Loss: 0.7655\n",
            ">>> Iteration: 1000/3287, Train Loss: 0.7727\n",
            ">>> Iteration: 1500/3287, Train Loss: 0.7800\n",
            ">>> Iteration: 2000/3287, Train Loss: 0.7848\n",
            ">>> Iteration: 2500/3287, Train Loss: 0.7884\n",
            ">>> Iteration: 3000/3287, Train Loss: 0.7922\n",
            "Epoch 25/20, Train Loss: 0.7941, Eval Loss: 1.8092\n",
            "Time taken for 1 epoch: 228s\n",
            ">>> Iteration: 500/3287, Train Loss: 0.7209\n",
            ">>> Iteration: 1000/3287, Train Loss: 0.7245\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-02bcd820e1ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnum_train_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdistributed_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnum_train_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    692\u001b[0m           \u001b[0;31m# Make `replicas` a flat list of values across all replicas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m           replicas.extend(\n\u001b[0;32m--> 694\u001b[0;31m               self._iterators[i].get_next_as_list_static_shapes(new_name))\n\u001b[0m\u001b[1;32m    695\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36mget_next_as_list_static_shapes\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_next_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_iterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_next_as_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2605\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2608\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxEv-yU30YIj"
      },
      "source": [
        "The training process is stopped here.\n",
        "\n",
        "Although the training loss is still decreasing, the evaluation loss has started to increase.\n",
        "\n",
        "We then proceed to save the model using keras' **model.save()**. For this to be possible, the model in question must only have one input variable for its **call** function(see model definition above).\n",
        "\n",
        "**save_options** defined below is necessary when training with TPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ_TkdEq0EKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd97a180-699f-4ca2-aae1-788b9cbe2525"
      },
      "source": [
        "save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "\n",
        "encoder_path = os.path.join(root_path, 'encoder_25ep')\n",
        "encoder.save(encoder_path, options=save_options)\n",
        "\n",
        "decoder_path = os.path.join(root_path, 'decoder_25ep')\n",
        "decoder.save(decoder_path, options=save_options)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/Chatbots/version-2/encoder_25ep/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/Chatbots/version-2/encoder_25ep/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/Chatbots/version-2/decoder_25ep/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/Chatbots/version-2/decoder_25ep/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZa0nLnc_f8K"
      },
      "source": [
        "### Inferencing\n",
        "\n",
        "A class is defined here for inference, both **greedy-search** and **beam-search** are implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRZvjF0K0EXS"
      },
      "source": [
        "class Chatbot:\n",
        "\tdef __init__(self, encoder, decoder, tokenizer, max_len=MAX_LEN):\n",
        "\t\tself.encoder = encoder\n",
        "\t\tself.decoder = decoder\n",
        "\t\tself.tokenizer = tokenizer\n",
        "\t\tself.max_len = max_len\n",
        "\n",
        "\tdef preprocess_text(self, text):\n",
        "\t\ttext = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "\t\t\n",
        "\t\ttext = re.sub(r'[\" \"]+', \" \", text)\n",
        "\n",
        "\t\ttext = re.sub(r\"[^a-zA-Z0-9?.!,]+\", \" \", text)\n",
        "\n",
        "\t\ttext = text.strip()\n",
        "\n",
        "\t\ttext = '<start> ' + text + ' <end>'\n",
        "\t\t\n",
        "\t\treturn text\n",
        "\n",
        "\tdef prepare_input(self, message):\n",
        "\t\tmessage = self.preprocess_text(message)\n",
        "\n",
        "\t\tsequence = self.tokenizer.texts_to_sequences([message])\n",
        "\n",
        "\t\tpadded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "\t\t\tsequence,\n",
        "\t\t\tmaxlen=self.max_len,\n",
        "\t\t\ttruncating='post')\n",
        "\n",
        "\t\ttensor = tf.convert_to_tensor(padded_sequence)\n",
        "\n",
        "\t\treturn tensor\n",
        "\n",
        "\tdef greedy_search(self, message):\n",
        "\t\ttensor = self.prepare_input(message)\n",
        "\n",
        "\t\tresponse = \"\"\n",
        "\n",
        "\t\tenc_output, hiddens = self.encoder(tensor)\n",
        "\n",
        "\t\tdec_in = (tf.expand_dims([self.tokenizer.word_index['<start>']], 0),\n",
        "\t\t\thiddens, enc_output)\n",
        "\n",
        "\t\tfor t in range(self.max_len):\n",
        "\t\t\tpred, hiddens, _ = self.decoder(dec_in)\n",
        "\n",
        "\t\t\tpred_id = tf.argmax(pred[0]).numpy()\n",
        "\n",
        "\t\t\tpred_word = self.tokenizer.index_word[pred_id]\n",
        "\n",
        "\t\t\tif pred_word == '<end>':\n",
        "\t\t\t\treturn message, response.strip()\n",
        "\n",
        "\t\t\tresponse += pred_word + ' '\n",
        "\n",
        "\t\t\tdec_in = (tf.expand_dims([pred_id], 0), hiddens, enc_output)\n",
        "\n",
        "\t\treturn message, response.strip()\n",
        "\n",
        "\tdef find_top_k(self, acc_val, ids, hiddens, enc_sequence, k):\n",
        "\t\tdec_in = (tf.expand_dims([ids[-1]], 0), hiddens, enc_sequence)\n",
        "\t\tpred, hiddens, _ = self.decoder(dec_in)\n",
        "\n",
        "\t\ttop_k = tf.math.top_k(pred, k=k)\n",
        "\t\ttop_vals = tf.nn.softmax(top_k.values).numpy()[0]\n",
        "\t\ttop_indices = top_k.indices.numpy()[0]\n",
        "\n",
        "\t\tcandidates = []\n",
        "\n",
        "\t\tfor val, id_ in zip(top_vals, top_indices):\n",
        "\t\t\tcandidates.append([val, ids + [id_], hiddens])\n",
        "\n",
        "\t\treturn candidates\n",
        "\n",
        "\tdef beam_search(self, message, k=5):\n",
        "\t\tstart_token = self.tokenizer.word_index['<start>']\n",
        "\t\tend_token = self.tokenizer.word_index['<end>']\n",
        "\n",
        "\t\ttensor = self.prepare_input(message)\n",
        "\n",
        "\t\tenc_sequence, hiddens = self.encoder(tensor)\n",
        "\n",
        "\t\tcandidates = self.find_top_k(1, [start_token], hiddens, enc_sequence, k)\n",
        "\n",
        "\t\twhile True:\n",
        "\t\t\tnext_candidates = []\n",
        "\t\t\tfor candidate in candidates:\n",
        "\t\t\t\tif len(candidate[1]) == self.max_len or candidate[1][-1] == end_token:\n",
        "\t\t\t\t\tnext_candidates.append(candidate)\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tnext_candidates.extend(self.find_top_k(*candidate, enc_sequence, k))\n",
        "\n",
        "\t\t\tcandidates = sorted(next_candidates, reverse=True)[:k]\n",
        "\n",
        "\t\t\tare_ended = []\n",
        "\t\t\tfor candidate in candidates:\n",
        "\t\t\t\tis_ended = len(candidate[1]) == self.max_len or candidate[1][-1] == end_token\n",
        "\t\t\t\tare_ended.append(is_ended)\n",
        "\n",
        "\t\t\tif all(are_ended):\n",
        "\t\t\t\tsequences = [cand[1][1:-1] for cand in candidates]\n",
        "\t\t\t\t# response = choice(self.tokenizer.sequences_to_texts(sequences))\n",
        "\t\t\t\tresponse = self.tokenizer.sequences_to_texts(sequences)[0]\n",
        "\t\t\t\treturn message, response, sequences[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye92TQR7l0DX"
      },
      "source": [
        "chatbot = Chatbot(encoder, decoder, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPMujCZVl0Gd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "d04c8678-b9f2-44cd-befb-121e345e86c6"
      },
      "source": [
        "while True:\n",
        "    message = input('You: ')\n",
        "    print(f'Bot: {chatbot.beam_search(message)[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bot: hey , hows it going ?\n",
            "Bot: what s the matter with that guy ?\n",
            "Bot: don t you ?\n",
            "Bot: how do you feel about that ?\n",
            "Bot: don t you want to talk about me ?\n",
            "Bot: he s a very sick guy , what is up .\n",
            "Bot: who the ?\n",
            "Bot: tell me more about yourself , please .\n",
            "Bot: , my family moved here so i am in the city .\n",
            "Bot: 6 and 4 . do you have any children ?\n",
            "Bot: that s nice . how many kids do you have ?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-30406b2e295b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Bot: {chatbot.beam_search(message)[1]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEg4_WmYl0JU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}